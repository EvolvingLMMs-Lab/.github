## LMMs-Lab: Building Multimodal Intelligence

We are a group of researchers on multimodal models. We wish to bring insights to community with our research.

Here're a few of our projects.

## [LMMS-Eval](https://github.com/EvolvingLMMs-Lab/lmms-eval)

<p align="center">
    <img src="assets/images/niah_output/LongVA-7B/heatmap.png" width="800">
</p>

We're on an exciting journey toward creating Artificial General Intelligence (AGI), much like the enthusiasm of the 1960s moon landing. This journey is powered by advanced large language models (LLMs) and large multimodal models (LMMs), which are complex systems capable of understanding, learning, and performing a wide variety of human tasks.

To gauge how advanced these models are, we use a variety of evaluation benchmarks. These benchmarks are tools that help us understand the capabilities of these models, showing us how close we are to achieving AGI. To address this challenge, we introduce lmms-eval, an evaluation framework meticulously crafted for consistent and efficient evaluation of LMM.
